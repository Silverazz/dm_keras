{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dm_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwShwTDN1lWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e5c3c07-1b3a-4ad6-9a5c-9a999287626c"
      },
      "source": [
        "##########################DM########################\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        " \n",
        "#Chargement des données de cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        " \n",
        "###Convert to float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        " \n",
        "###Normalize inputs from [0; 255] to [0; 1]\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        " \n",
        "###Préparation pour couche neuronnale\n",
        "num_classes = 10\n",
        " \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        " \n",
        "###Learning rate\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        " \n",
        "def cnn():\n",
        "    weight_decay = 1e-4\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "     \n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "     \n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('elu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "     \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "    return model\n",
        " \n",
        "###creation du reseau\n",
        " \n",
        "model = cnn()\n",
        "model.summary()\n",
        " \n",
        "###entrainement\n",
        "batch_size=32\n",
        "epochs=150\n",
        " \n",
        "#construct the training image generator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        " \n",
        "#stops when acc doesn't change anymore\n",
        "earlystopper = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        " \n",
        "# phase 1 train / Data augmentation\n",
        "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        " \n",
        "# phase 2 train / Normal\n",
        "hist2 = model.fit(x_train, y_train,\n",
        "            validation_data=(x_test, y_test),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size)   \n",
        " \n",
        "###evaluation\n",
        " \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "#plot accuracies\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist2.history['acc'])\n",
        "#plt.plot(hist.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/125\n",
            "1562/1562 [==============================] - 39s 25ms/step - loss: 1.8985 - acc: 0.4272 - val_loss: 1.5347 - val_acc: 0.5480\n",
            "Epoch 2/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 1.2766 - acc: 0.5950 - val_loss: 1.1258 - val_acc: 0.6591\n",
            "Epoch 3/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 1.0958 - acc: 0.6548 - val_loss: 0.8851 - val_acc: 0.7343\n",
            "Epoch 4/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 1.0060 - acc: 0.6903 - val_loss: 0.9484 - val_acc: 0.7217\n",
            "Epoch 5/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9431 - acc: 0.7155 - val_loss: 0.9939 - val_acc: 0.7046\n",
            "Epoch 6/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.9065 - acc: 0.7324 - val_loss: 0.9462 - val_acc: 0.7327\n",
            "Epoch 7/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.8744 - acc: 0.7447 - val_loss: 0.8474 - val_acc: 0.7570\n",
            "Epoch 8/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8481 - acc: 0.7561 - val_loss: 0.8178 - val_acc: 0.7736\n",
            "Epoch 9/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8279 - acc: 0.7643 - val_loss: 0.8312 - val_acc: 0.7709\n",
            "Epoch 10/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8162 - acc: 0.7710 - val_loss: 0.8344 - val_acc: 0.7691\n",
            "Epoch 11/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8012 - acc: 0.7756 - val_loss: 0.8895 - val_acc: 0.7608\n",
            "Epoch 12/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7925 - acc: 0.7794 - val_loss: 0.7670 - val_acc: 0.7975\n",
            "Epoch 13/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7853 - acc: 0.7837 - val_loss: 0.7479 - val_acc: 0.7976\n",
            "Epoch 14/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7805 - acc: 0.7856 - val_loss: 0.7959 - val_acc: 0.7883\n",
            "Epoch 15/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7718 - acc: 0.7901 - val_loss: 0.7008 - val_acc: 0.8174\n",
            "Epoch 16/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7660 - acc: 0.7943 - val_loss: 0.7364 - val_acc: 0.8057\n",
            "Epoch 17/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7561 - acc: 0.7947 - val_loss: 0.7576 - val_acc: 0.7998\n",
            "Epoch 18/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7524 - acc: 0.7986 - val_loss: 0.7958 - val_acc: 0.7858\n",
            "Epoch 19/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7473 - acc: 0.8007 - val_loss: 0.8170 - val_acc: 0.7968\n",
            "Epoch 20/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7444 - acc: 0.8018 - val_loss: 0.7599 - val_acc: 0.8132\n",
            "Epoch 21/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7417 - acc: 0.8045 - val_loss: 0.7458 - val_acc: 0.8131\n",
            "Epoch 22/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7367 - acc: 0.8048 - val_loss: 0.7433 - val_acc: 0.8101\n",
            "Epoch 23/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7337 - acc: 0.8070 - val_loss: 0.7708 - val_acc: 0.8094\n",
            "Epoch 24/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7282 - acc: 0.8093 - val_loss: 0.7100 - val_acc: 0.8207\n",
            "Epoch 25/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7217 - acc: 0.8131 - val_loss: 0.7148 - val_acc: 0.8196\n",
            "Epoch 26/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7222 - acc: 0.8125 - val_loss: 0.8183 - val_acc: 0.7908\n",
            "Epoch 27/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7199 - acc: 0.8137 - val_loss: 0.6858 - val_acc: 0.8357\n",
            "Epoch 28/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7175 - acc: 0.8159 - val_loss: 0.6889 - val_acc: 0.8331\n",
            "Epoch 29/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7163 - acc: 0.8139 - val_loss: 0.7034 - val_acc: 0.8257\n",
            "Epoch 30/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7092 - acc: 0.8173 - val_loss: 0.7400 - val_acc: 0.8197\n",
            "Epoch 31/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7084 - acc: 0.8185 - val_loss: 0.7187 - val_acc: 0.8189\n",
            "Epoch 32/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7088 - acc: 0.8164 - val_loss: 0.7050 - val_acc: 0.8288\n",
            "Epoch 33/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7097 - acc: 0.8176 - val_loss: 0.6024 - val_acc: 0.8583\n",
            "Epoch 34/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7032 - acc: 0.8187 - val_loss: 0.6851 - val_acc: 0.8352\n",
            "Epoch 35/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7066 - acc: 0.8188 - val_loss: 0.7648 - val_acc: 0.8053\n",
            "Epoch 36/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6978 - acc: 0.8235 - val_loss: 0.6457 - val_acc: 0.8453\n",
            "Epoch 37/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7029 - acc: 0.8207 - val_loss: 0.7163 - val_acc: 0.8250\n",
            "Epoch 38/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6893 - acc: 0.8252 - val_loss: 0.6537 - val_acc: 0.8446\n",
            "Epoch 39/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6934 - acc: 0.8250 - val_loss: 0.6617 - val_acc: 0.8406\n",
            "Epoch 40/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6937 - acc: 0.8236 - val_loss: 0.6483 - val_acc: 0.8482\n",
            "Epoch 41/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6916 - acc: 0.8239 - val_loss: 0.6342 - val_acc: 0.8442\n",
            "Epoch 42/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6911 - acc: 0.8226 - val_loss: 0.7866 - val_acc: 0.8062\n",
            "Epoch 43/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6916 - acc: 0.8248 - val_loss: 0.6229 - val_acc: 0.8481\n",
            "Epoch 44/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6854 - acc: 0.8258 - val_loss: 0.6399 - val_acc: 0.8466\n",
            "Epoch 45/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6871 - acc: 0.8281 - val_loss: 0.7712 - val_acc: 0.8016\n",
            "Epoch 46/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6815 - acc: 0.8264 - val_loss: 0.5982 - val_acc: 0.8601\n",
            "Epoch 47/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6839 - acc: 0.8264 - val_loss: 0.7176 - val_acc: 0.8257\n",
            "Epoch 48/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6831 - acc: 0.8280 - val_loss: 0.6868 - val_acc: 0.8363\n",
            "Epoch 49/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6798 - acc: 0.8288 - val_loss: 0.7199 - val_acc: 0.8220\n",
            "Epoch 50/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6745 - acc: 0.8303 - val_loss: 0.6687 - val_acc: 0.8424\n",
            "Epoch 51/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6834 - acc: 0.8265 - val_loss: 0.7041 - val_acc: 0.8319\n",
            "Epoch 52/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6769 - acc: 0.8309 - val_loss: 0.6432 - val_acc: 0.8468\n",
            "Epoch 53/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6716 - acc: 0.8306 - val_loss: 0.6469 - val_acc: 0.8428\n",
            "Epoch 54/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6714 - acc: 0.8308 - val_loss: 0.6378 - val_acc: 0.8464\n",
            "Epoch 55/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6772 - acc: 0.8321 - val_loss: 0.6413 - val_acc: 0.8514\n",
            "Epoch 56/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6777 - acc: 0.8301 - val_loss: 0.6491 - val_acc: 0.8460\n",
            "Epoch 57/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6711 - acc: 0.8313 - val_loss: 0.6646 - val_acc: 0.8423\n",
            "Epoch 58/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6768 - acc: 0.8294 - val_loss: 0.6554 - val_acc: 0.8408\n",
            "Epoch 59/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6695 - acc: 0.8317 - val_loss: 0.6692 - val_acc: 0.8412\n",
            "Epoch 60/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6662 - acc: 0.8321 - val_loss: 0.6512 - val_acc: 0.8437\n",
            "Epoch 61/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6684 - acc: 0.8320 - val_loss: 0.6697 - val_acc: 0.8389\n",
            "Epoch 62/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6666 - acc: 0.8315 - val_loss: 0.7191 - val_acc: 0.8263\n",
            "Epoch 63/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6643 - acc: 0.8342 - val_loss: 0.5937 - val_acc: 0.8619\n",
            "Epoch 64/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6651 - acc: 0.8338 - val_loss: 0.6383 - val_acc: 0.8472\n",
            "Epoch 65/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6634 - acc: 0.8348 - val_loss: 0.6089 - val_acc: 0.8546\n",
            "Epoch 66/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6572 - acc: 0.8364 - val_loss: 0.6393 - val_acc: 0.8502\n",
            "Epoch 67/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6670 - acc: 0.8335 - val_loss: 0.6375 - val_acc: 0.8486\n",
            "Epoch 68/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6621 - acc: 0.8359 - val_loss: 0.6548 - val_acc: 0.8435\n",
            "Epoch 69/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6618 - acc: 0.8342 - val_loss: 0.6767 - val_acc: 0.8372\n",
            "Epoch 70/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6642 - acc: 0.8343 - val_loss: 0.6085 - val_acc: 0.8568\n",
            "Epoch 71/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6636 - acc: 0.8355 - val_loss: 0.6962 - val_acc: 0.8364\n",
            "Epoch 72/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6617 - acc: 0.8343 - val_loss: 0.6483 - val_acc: 0.8500\n",
            "Epoch 73/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6602 - acc: 0.8352 - val_loss: 0.6944 - val_acc: 0.8309\n",
            "Epoch 74/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6600 - acc: 0.8358 - val_loss: 0.6817 - val_acc: 0.8377\n",
            "Epoch 75/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6601 - acc: 0.8370 - val_loss: 0.6506 - val_acc: 0.8496\n",
            "Epoch 76/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6605 - acc: 0.8364 - val_loss: 0.6362 - val_acc: 0.8494\n",
            "Epoch 77/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6039 - acc: 0.8524 - val_loss: 0.5917 - val_acc: 0.8645\n",
            "Epoch 78/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5874 - acc: 0.8557 - val_loss: 0.6033 - val_acc: 0.8570\n",
            "Epoch 79/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5701 - acc: 0.8600 - val_loss: 0.6304 - val_acc: 0.8486\n",
            "Epoch 80/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5584 - acc: 0.8636 - val_loss: 0.5695 - val_acc: 0.8673\n",
            "Epoch 81/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5569 - acc: 0.8620 - val_loss: 0.5894 - val_acc: 0.8631\n",
            "Epoch 82/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5579 - acc: 0.8608 - val_loss: 0.5796 - val_acc: 0.8615\n",
            "Epoch 83/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5482 - acc: 0.8625 - val_loss: 0.5462 - val_acc: 0.8732\n",
            "Epoch 84/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5423 - acc: 0.8648 - val_loss: 0.5306 - val_acc: 0.8760\n",
            "Epoch 85/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5417 - acc: 0.8645 - val_loss: 0.5492 - val_acc: 0.8708\n",
            "Epoch 86/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5381 - acc: 0.8644 - val_loss: 0.5203 - val_acc: 0.8786\n",
            "Epoch 87/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5358 - acc: 0.8659 - val_loss: 0.5616 - val_acc: 0.8640\n",
            "Epoch 88/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5377 - acc: 0.8628 - val_loss: 0.5728 - val_acc: 0.8613\n",
            "Epoch 89/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5298 - acc: 0.8654 - val_loss: 0.5490 - val_acc: 0.8676\n",
            "Epoch 90/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5305 - acc: 0.8660 - val_loss: 0.5326 - val_acc: 0.8722\n",
            "Epoch 91/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5290 - acc: 0.8654 - val_loss: 0.5798 - val_acc: 0.8635\n",
            "Epoch 92/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5266 - acc: 0.8680 - val_loss: 0.5785 - val_acc: 0.8604\n",
            "Epoch 93/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5240 - acc: 0.8674 - val_loss: 0.5365 - val_acc: 0.8681\n",
            "Epoch 94/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5214 - acc: 0.8662 - val_loss: 0.5382 - val_acc: 0.8722\n",
            "Epoch 95/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5216 - acc: 0.8679 - val_loss: 0.5336 - val_acc: 0.8719\n",
            "Epoch 96/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5195 - acc: 0.8671 - val_loss: 0.5191 - val_acc: 0.8757\n",
            "Epoch 97/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5192 - acc: 0.8657 - val_loss: 0.5215 - val_acc: 0.8713\n",
            "Epoch 98/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5139 - acc: 0.8682 - val_loss: 0.5571 - val_acc: 0.8673\n",
            "Epoch 99/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.5235 - acc: 0.8655 - val_loss: 0.6095 - val_acc: 0.8490\n",
            "Epoch 100/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5166 - acc: 0.8671 - val_loss: 0.5546 - val_acc: 0.8638\n",
            "Epoch 101/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.5167 - acc: 0.8664 - val_loss: 0.5089 - val_acc: 0.8759\n",
            "Epoch 102/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4931 - acc: 0.8751 - val_loss: 0.5453 - val_acc: 0.8664\n",
            "Epoch 103/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4852 - acc: 0.8770 - val_loss: 0.4915 - val_acc: 0.8806\n",
            "Epoch 104/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4792 - acc: 0.8783 - val_loss: 0.5210 - val_acc: 0.8731\n",
            "Epoch 105/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4698 - acc: 0.8812 - val_loss: 0.5234 - val_acc: 0.8736\n",
            "Epoch 106/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4733 - acc: 0.8790 - val_loss: 0.4905 - val_acc: 0.8829\n",
            "Epoch 107/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4687 - acc: 0.8803 - val_loss: 0.5020 - val_acc: 0.8753\n",
            "Epoch 108/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4664 - acc: 0.8807 - val_loss: 0.5199 - val_acc: 0.8719\n",
            "Epoch 109/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4663 - acc: 0.8803 - val_loss: 0.5015 - val_acc: 0.8777\n",
            "Epoch 110/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4627 - acc: 0.8801 - val_loss: 0.5488 - val_acc: 0.8641\n",
            "Epoch 111/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4603 - acc: 0.8821 - val_loss: 0.4938 - val_acc: 0.8792\n",
            "Epoch 112/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4579 - acc: 0.8824 - val_loss: 0.4807 - val_acc: 0.8824\n",
            "Epoch 113/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4561 - acc: 0.8827 - val_loss: 0.4987 - val_acc: 0.8775\n",
            "Epoch 114/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.4557 - acc: 0.8810 - val_loss: 0.4996 - val_acc: 0.8764\n",
            "Epoch 115/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4575 - acc: 0.8813 - val_loss: 0.5505 - val_acc: 0.8644\n",
            "Epoch 116/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4453 - acc: 0.8857 - val_loss: 0.5124 - val_acc: 0.8725\n",
            "Epoch 117/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4518 - acc: 0.8827 - val_loss: 0.4724 - val_acc: 0.8890\n",
            "Epoch 118/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4512 - acc: 0.8821 - val_loss: 0.4666 - val_acc: 0.8833\n",
            "Epoch 119/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4517 - acc: 0.8820 - val_loss: 0.4755 - val_acc: 0.8777\n",
            "Epoch 120/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4517 - acc: 0.8832 - val_loss: 0.5107 - val_acc: 0.8708\n",
            "Epoch 121/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4420 - acc: 0.8867 - val_loss: 0.5017 - val_acc: 0.8743\n",
            "Epoch 122/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4484 - acc: 0.8833 - val_loss: 0.4889 - val_acc: 0.8770\n",
            "Epoch 123/125\n",
            "1562/1562 [==============================] - 32s 20ms/step - loss: 0.4457 - acc: 0.8832 - val_loss: 0.4885 - val_acc: 0.8787\n",
            "Epoch 124/125\n",
            "1562/1562 [==============================] - 32s 21ms/step - loss: 0.4418 - acc: 0.8854 - val_loss: 0.4836 - val_acc: 0.8859\n",
            "Epoch 125/125\n",
            "1562/1562 [==============================] - 31s 20ms/step - loss: 0.4490 - acc: 0.8832 - val_loss: 0.4766 - val_acc: 0.8839\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.3601 - acc: 0.9130 - val_loss: 0.4070 - val_acc: 0.9014\n",
            "Epoch 2/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.3408 - acc: 0.9196 - val_loss: 0.4101 - val_acc: 0.9003\n",
            "Epoch 3/150\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 0.3255 - acc: 0.9241 - val_loss: 0.4143 - val_acc: 0.8987\n",
            "Epoch 4/150\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 0.3105 - acc: 0.9286 - val_loss: 0.4138 - val_acc: 0.9047\n",
            "Epoch 5/150\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 0.3047 - acc: 0.9295 - val_loss: 0.4198 - val_acc: 0.8975\n",
            "Epoch 6/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2945 - acc: 0.9332 - val_loss: 0.4191 - val_acc: 0.9014\n",
            "Epoch 7/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2909 - acc: 0.9339 - val_loss: 0.4300 - val_acc: 0.8971\n",
            "Epoch 8/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2812 - acc: 0.9365 - val_loss: 0.4323 - val_acc: 0.8990\n",
            "Epoch 9/150\n",
            "50000/50000 [==============================] - 19s 376us/step - loss: 0.2788 - acc: 0.9379 - val_loss: 0.4212 - val_acc: 0.9024\n",
            "Epoch 10/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2750 - acc: 0.9396 - val_loss: 0.4254 - val_acc: 0.9017\n",
            "Epoch 11/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.2718 - acc: 0.9405 - val_loss: 0.4181 - val_acc: 0.9026\n",
            "Epoch 12/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2640 - acc: 0.9431 - val_loss: 0.4341 - val_acc: 0.9012\n",
            "Epoch 13/150\n",
            "50000/50000 [==============================] - 19s 375us/step - loss: 0.2641 - acc: 0.9427 - val_loss: 0.4271 - val_acc: 0.9020\n",
            "Epoch 14/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2608 - acc: 0.9438 - val_loss: 0.4516 - val_acc: 0.8969\n",
            "Epoch 15/150\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 0.2523 - acc: 0.9464 - val_loss: 0.4395 - val_acc: 0.9022\n",
            "Epoch 16/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2529 - acc: 0.9467 - val_loss: 0.4356 - val_acc: 0.8985\n",
            "Epoch 17/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2497 - acc: 0.9479 - val_loss: 0.4694 - val_acc: 0.8948\n",
            "Epoch 18/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2517 - acc: 0.9458 - val_loss: 0.4482 - val_acc: 0.9003\n",
            "Epoch 19/150\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 0.2492 - acc: 0.9471 - val_loss: 0.4422 - val_acc: 0.8991\n",
            "Epoch 20/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2442 - acc: 0.9480 - val_loss: 0.4599 - val_acc: 0.8983\n",
            "Epoch 21/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2424 - acc: 0.9500 - val_loss: 0.4610 - val_acc: 0.8970\n",
            "Epoch 22/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2378 - acc: 0.9509 - val_loss: 0.4635 - val_acc: 0.8966\n",
            "Epoch 23/150\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 0.2357 - acc: 0.9523 - val_loss: 0.4523 - val_acc: 0.9034\n",
            "Epoch 24/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2387 - acc: 0.9503 - val_loss: 0.4474 - val_acc: 0.9021\n",
            "Epoch 25/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2334 - acc: 0.9527 - val_loss: 0.4735 - val_acc: 0.8969\n",
            "Epoch 26/150\n",
            "50000/50000 [==============================] - 19s 377us/step - loss: 0.2317 - acc: 0.9540 - val_loss: 0.4627 - val_acc: 0.8994\n",
            "Epoch 27/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2336 - acc: 0.9523 - val_loss: 0.4665 - val_acc: 0.8989\n",
            "Epoch 28/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2341 - acc: 0.9525 - val_loss: 0.4789 - val_acc: 0.8962\n",
            "Epoch 29/150\n",
            "50000/50000 [==============================] - 19s 375us/step - loss: 0.2301 - acc: 0.9528 - val_loss: 0.4639 - val_acc: 0.9002\n",
            "Epoch 30/150\n",
            "50000/50000 [==============================] - 19s 374us/step - loss: 0.2266 - acc: 0.9541 - val_loss: 0.4711 - val_acc: 0.8951\n",
            "Epoch 31/150\n",
            "50000/50000 [==============================] - 19s 385us/step - loss: 0.2314 - acc: 0.9529 - val_loss: 0.4663 - val_acc: 0.8960\n",
            "Epoch 32/150\n",
            "50000/50000 [==============================] - 19s 373us/step - loss: 0.2274 - acc: 0.9548 - val_loss: 0.4804 - val_acc: 0.8965\n",
            "Epoch 33/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2285 - acc: 0.9552 - val_loss: 0.4839 - val_acc: 0.8930\n",
            "Epoch 34/150\n",
            "50000/50000 [==============================] - 19s 375us/step - loss: 0.2265 - acc: 0.9540 - val_loss: 0.4667 - val_acc: 0.8977\n",
            "Epoch 35/150\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 0.2282 - acc: 0.9535 - val_loss: 0.4924 - val_acc: 0.8953\n",
            "Epoch 36/150\n",
            "50000/50000 [==============================] - 19s 377us/step - loss: 0.2245 - acc: 0.9553 - val_loss: 0.4905 - val_acc: 0.8944\n",
            "Epoch 37/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2244 - acc: 0.9562 - val_loss: 0.4681 - val_acc: 0.8970\n",
            "Epoch 38/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2214 - acc: 0.9569 - val_loss: 0.4779 - val_acc: 0.8947\n",
            "Epoch 39/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2197 - acc: 0.9572 - val_loss: 0.4692 - val_acc: 0.8947\n",
            "Epoch 40/150\n",
            "50000/50000 [==============================] - 19s 377us/step - loss: 0.2167 - acc: 0.9582 - val_loss: 0.4744 - val_acc: 0.8972\n",
            "Epoch 41/150\n",
            "50000/50000 [==============================] - 19s 377us/step - loss: 0.2203 - acc: 0.9566 - val_loss: 0.4809 - val_acc: 0.8948\n",
            "Epoch 42/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2185 - acc: 0.9568 - val_loss: 0.4959 - val_acc: 0.8904\n",
            "Epoch 43/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2190 - acc: 0.9585 - val_loss: 0.4879 - val_acc: 0.8958\n",
            "Epoch 44/150\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 0.2195 - acc: 0.9587 - val_loss: 0.4906 - val_acc: 0.8945\n",
            "Epoch 45/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2169 - acc: 0.9586 - val_loss: 0.4849 - val_acc: 0.8954\n",
            "Epoch 46/150\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 0.2130 - acc: 0.9581 - val_loss: 0.4885 - val_acc: 0.8939\n",
            "Epoch 47/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2137 - acc: 0.9590 - val_loss: 0.4940 - val_acc: 0.8947\n",
            "Epoch 48/150\n",
            "50000/50000 [==============================] - 19s 386us/step - loss: 0.2121 - acc: 0.9596 - val_loss: 0.4907 - val_acc: 0.8952\n",
            "Epoch 49/150\n",
            "50000/50000 [==============================] - 19s 385us/step - loss: 0.2149 - acc: 0.9591 - val_loss: 0.4864 - val_acc: 0.8964\n",
            "Epoch 50/150\n",
            "50000/50000 [==============================] - 19s 379us/step - loss: 0.2152 - acc: 0.9576 - val_loss: 0.4752 - val_acc: 0.8973\n",
            "Epoch 51/150\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 0.2164 - acc: 0.9586 - val_loss: 0.4673 - val_acc: 0.8994\n",
            "Epoch 52/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2159 - acc: 0.9577 - val_loss: 0.4891 - val_acc: 0.8932\n",
            "Epoch 53/150\n",
            "50000/50000 [==============================] - 19s 376us/step - loss: 0.2135 - acc: 0.9593 - val_loss: 0.4669 - val_acc: 0.8975\n",
            "Epoch 54/150\n",
            "50000/50000 [==============================] - 19s 378us/step - loss: 0.2131 - acc: 0.9594 - val_loss: 0.4616 - val_acc: 0.9011\n",
            "Epoch 55/150\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 0.2146 - acc: 0.9585 - val_loss: 0.4944 - val_acc: 0.8984\n",
            "Epoch 56/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.2075 - acc: 0.9610 - val_loss: 0.4947 - val_acc: 0.8971\n",
            "Epoch 57/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.2108 - acc: 0.9601 - val_loss: 0.4930 - val_acc: 0.8920\n",
            "Epoch 58/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.2133 - acc: 0.9593 - val_loss: 0.4781 - val_acc: 0.8974\n",
            "Epoch 59/150\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 0.2084 - acc: 0.9609 - val_loss: 0.4810 - val_acc: 0.8960\n",
            "Epoch 60/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.2087 - acc: 0.9603 - val_loss: 0.4897 - val_acc: 0.8948\n",
            "Epoch 61/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.2094 - acc: 0.9609 - val_loss: 0.4846 - val_acc: 0.8936\n",
            "Epoch 62/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.2087 - acc: 0.9613 - val_loss: 0.4822 - val_acc: 0.8955\n",
            "Epoch 63/150\n",
            "50000/50000 [==============================] - 19s 385us/step - loss: 0.2071 - acc: 0.9605 - val_loss: 0.5011 - val_acc: 0.8927\n",
            "Epoch 64/150\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.2100 - acc: 0.9612 - val_loss: 0.5040 - val_acc: 0.8917\n",
            "Epoch 65/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.2031 - acc: 0.9633 - val_loss: 0.4896 - val_acc: 0.8948\n",
            "Epoch 66/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.2104 - acc: 0.9609 - val_loss: 0.5018 - val_acc: 0.8934\n",
            "Epoch 67/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.2042 - acc: 0.9622 - val_loss: 0.5055 - val_acc: 0.8945\n",
            "Epoch 68/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.2026 - acc: 0.9624 - val_loss: 0.5084 - val_acc: 0.8933\n",
            "Epoch 69/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.2051 - acc: 0.9618 - val_loss: 0.4865 - val_acc: 0.8952\n",
            "Epoch 70/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.2033 - acc: 0.9631 - val_loss: 0.5093 - val_acc: 0.8939\n",
            "Epoch 71/150\n",
            "50000/50000 [==============================] - 19s 385us/step - loss: 0.2040 - acc: 0.9628 - val_loss: 0.4998 - val_acc: 0.8975\n",
            "Epoch 72/150\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.2036 - acc: 0.9625 - val_loss: 0.5129 - val_acc: 0.8905\n",
            "Epoch 73/150\n",
            "50000/50000 [==============================] - 19s 389us/step - loss: 0.2027 - acc: 0.9629 - val_loss: 0.5047 - val_acc: 0.8928\n",
            "Epoch 74/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.1996 - acc: 0.9635 - val_loss: 0.5283 - val_acc: 0.8931\n",
            "Epoch 75/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.2049 - acc: 0.9621 - val_loss: 0.5119 - val_acc: 0.8946\n",
            "Epoch 76/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.2055 - acc: 0.9617 - val_loss: 0.4976 - val_acc: 0.8979\n",
            "Epoch 77/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.2030 - acc: 0.9625 - val_loss: 0.4989 - val_acc: 0.8924\n",
            "Epoch 78/150\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 0.2009 - acc: 0.9632 - val_loss: 0.4930 - val_acc: 0.8949\n",
            "Epoch 79/150\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.2030 - acc: 0.9625 - val_loss: 0.5004 - val_acc: 0.8939\n",
            "Epoch 80/150\n",
            "50000/50000 [==============================] - 19s 389us/step - loss: 0.2031 - acc: 0.9627 - val_loss: 0.5303 - val_acc: 0.8935\n",
            "Epoch 81/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.2021 - acc: 0.9622 - val_loss: 0.5408 - val_acc: 0.8912\n",
            "Epoch 82/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1981 - acc: 0.9643 - val_loss: 0.5122 - val_acc: 0.8955\n",
            "Epoch 83/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.2031 - acc: 0.9614 - val_loss: 0.5082 - val_acc: 0.8933\n",
            "Epoch 84/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.2004 - acc: 0.9626 - val_loss: 0.5024 - val_acc: 0.8946\n",
            "Epoch 85/150\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.1996 - acc: 0.9644 - val_loss: 0.4981 - val_acc: 0.8950\n",
            "Epoch 86/150\n",
            "50000/50000 [==============================] - 19s 386us/step - loss: 0.2006 - acc: 0.9636 - val_loss: 0.5023 - val_acc: 0.8942\n",
            "Epoch 87/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1953 - acc: 0.9645 - val_loss: 0.5107 - val_acc: 0.8918\n",
            "Epoch 88/150\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 0.1977 - acc: 0.9641 - val_loss: 0.5064 - val_acc: 0.8908\n",
            "Epoch 89/150\n",
            "50000/50000 [==============================] - 19s 390us/step - loss: 0.1995 - acc: 0.9639 - val_loss: 0.5343 - val_acc: 0.8913\n",
            "Epoch 90/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1978 - acc: 0.9634 - val_loss: 0.4974 - val_acc: 0.8926\n",
            "Epoch 91/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.1983 - acc: 0.9639 - val_loss: 0.5105 - val_acc: 0.8951\n",
            "Epoch 92/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1945 - acc: 0.9643 - val_loss: 0.5274 - val_acc: 0.8897\n",
            "Epoch 93/150\n",
            "50000/50000 [==============================] - 19s 385us/step - loss: 0.1967 - acc: 0.9647 - val_loss: 0.5180 - val_acc: 0.8931\n",
            "Epoch 94/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1982 - acc: 0.9644 - val_loss: 0.4956 - val_acc: 0.8949\n",
            "Epoch 95/150\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.1966 - acc: 0.9635 - val_loss: 0.5121 - val_acc: 0.8965\n",
            "Epoch 96/150\n",
            "50000/50000 [==============================] - 19s 389us/step - loss: 0.1960 - acc: 0.9642 - val_loss: 0.4984 - val_acc: 0.8967\n",
            "Epoch 97/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1951 - acc: 0.9638 - val_loss: 0.5152 - val_acc: 0.8924\n",
            "Epoch 98/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1944 - acc: 0.9650 - val_loss: 0.5189 - val_acc: 0.8973\n",
            "Epoch 99/150\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 0.1951 - acc: 0.9647 - val_loss: 0.5085 - val_acc: 0.8939\n",
            "Epoch 100/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1930 - acc: 0.9656 - val_loss: 0.5052 - val_acc: 0.8941\n",
            "Epoch 101/150\n",
            "50000/50000 [==============================] - 19s 386us/step - loss: 0.1947 - acc: 0.9660 - val_loss: 0.4848 - val_acc: 0.8995\n",
            "Epoch 102/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.1941 - acc: 0.9640 - val_loss: 0.5129 - val_acc: 0.8966\n",
            "Epoch 103/150\n",
            "50000/50000 [==============================] - 20s 390us/step - loss: 0.1908 - acc: 0.9666 - val_loss: 0.5130 - val_acc: 0.8932\n",
            "Epoch 104/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1928 - acc: 0.9665 - val_loss: 0.5150 - val_acc: 0.8918\n",
            "Epoch 105/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.1936 - acc: 0.9644 - val_loss: 0.4952 - val_acc: 0.8958\n",
            "Epoch 106/150\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.1922 - acc: 0.9656 - val_loss: 0.5058 - val_acc: 0.8927\n",
            "Epoch 107/150\n",
            "50000/50000 [==============================] - 19s 390us/step - loss: 0.1911 - acc: 0.9658 - val_loss: 0.5046 - val_acc: 0.8940\n",
            "Epoch 108/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.1924 - acc: 0.9653 - val_loss: 0.5071 - val_acc: 0.8940\n",
            "Epoch 109/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.1938 - acc: 0.9655 - val_loss: 0.5226 - val_acc: 0.8910\n",
            "Epoch 110/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1897 - acc: 0.9655 - val_loss: 0.5106 - val_acc: 0.8940\n",
            "Epoch 111/150\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 0.1929 - acc: 0.9655 - val_loss: 0.5103 - val_acc: 0.8938\n",
            "Epoch 112/150\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.1926 - acc: 0.9650 - val_loss: 0.5703 - val_acc: 0.8835\n",
            "Epoch 113/150\n",
            "50000/50000 [==============================] - 20s 393us/step - loss: 0.1934 - acc: 0.9648 - val_loss: 0.5189 - val_acc: 0.8921\n",
            "Epoch 114/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1943 - acc: 0.9646 - val_loss: 0.4878 - val_acc: 0.8975\n",
            "Epoch 115/150\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.1890 - acc: 0.9667 - val_loss: 0.5193 - val_acc: 0.8938\n",
            "Epoch 116/150\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.1947 - acc: 0.9647 - val_loss: 0.5084 - val_acc: 0.8956\n",
            "Epoch 117/150\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.1898 - acc: 0.9669 - val_loss: 0.4994 - val_acc: 0.8948\n",
            "Epoch 118/150\n",
            "50000/50000 [==============================] - 20s 396us/step - loss: 0.1897 - acc: 0.9662 - val_loss: 0.4924 - val_acc: 0.8955\n",
            "Epoch 119/150\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.1897 - acc: 0.9669 - val_loss: 0.5160 - val_acc: 0.8904\n",
            "Epoch 120/150\n",
            "50000/50000 [==============================] - 20s 398us/step - loss: 0.1872 - acc: 0.9669 - val_loss: 0.5397 - val_acc: 0.8922\n",
            "Epoch 121/150\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.1930 - acc: 0.9658 - val_loss: 0.5018 - val_acc: 0.8954\n",
            "Epoch 122/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.1930 - acc: 0.9652 - val_loss: 0.5127 - val_acc: 0.8942\n",
            "Epoch 123/150\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.1891 - acc: 0.9670 - val_loss: 0.5144 - val_acc: 0.8928\n",
            "Epoch 124/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.1854 - acc: 0.9674 - val_loss: 0.5363 - val_acc: 0.8888\n",
            "Epoch 125/150\n",
            "50000/50000 [==============================] - 20s 394us/step - loss: 0.1903 - acc: 0.9661 - val_loss: 0.4971 - val_acc: 0.8935\n",
            "Epoch 126/150\n",
            "50000/50000 [==============================] - 20s 390us/step - loss: 0.1837 - acc: 0.9677 - val_loss: 0.5066 - val_acc: 0.8930\n",
            "Epoch 127/150\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 0.1924 - acc: 0.9647 - val_loss: 0.4893 - val_acc: 0.8968\n",
            "Epoch 128/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.1837 - acc: 0.9675 - val_loss: 0.5205 - val_acc: 0.8939\n",
            "Epoch 129/150\n",
            "50000/50000 [==============================] - 19s 389us/step - loss: 0.1856 - acc: 0.9675 - val_loss: 0.4919 - val_acc: 0.8965\n",
            "Epoch 130/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1864 - acc: 0.9673 - val_loss: 0.5424 - val_acc: 0.8921\n",
            "Epoch 131/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.1861 - acc: 0.9670 - val_loss: 0.5016 - val_acc: 0.8952\n",
            "Epoch 132/150\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 0.1869 - acc: 0.9671 - val_loss: 0.5137 - val_acc: 0.8934\n",
            "Epoch 133/150\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.1884 - acc: 0.9666 - val_loss: 0.4940 - val_acc: 0.8937\n",
            "Epoch 134/150\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.1865 - acc: 0.9669 - val_loss: 0.5021 - val_acc: 0.8911\n",
            "Epoch 135/150\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 0.1863 - acc: 0.9667 - val_loss: 0.5158 - val_acc: 0.8956\n",
            "Epoch 136/150\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 0.1831 - acc: 0.9679 - val_loss: 0.5228 - val_acc: 0.8908\n",
            "Epoch 137/150\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.1875 - acc: 0.9662 - val_loss: 0.4985 - val_acc: 0.8982\n",
            "Epoch 138/150\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 0.1835 - acc: 0.9674 - val_loss: 0.5096 - val_acc: 0.8938\n",
            "Epoch 139/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1810 - acc: 0.9679 - val_loss: 0.5239 - val_acc: 0.8929\n",
            "Epoch 140/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1851 - acc: 0.9677 - val_loss: 0.5129 - val_acc: 0.8941\n",
            "Epoch 141/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1819 - acc: 0.9685 - val_loss: 0.5172 - val_acc: 0.8950\n",
            "Epoch 142/150\n",
            "50000/50000 [==============================] - 20s 397us/step - loss: 0.1809 - acc: 0.9683 - val_loss: 0.5252 - val_acc: 0.8907\n",
            "Epoch 143/150\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.1859 - acc: 0.9670 - val_loss: 0.5180 - val_acc: 0.8927\n",
            "Epoch 144/150\n",
            "50000/50000 [==============================] - 20s 399us/step - loss: 0.1814 - acc: 0.9673 - val_loss: 0.5171 - val_acc: 0.8898\n",
            "Epoch 145/150\n",
            "50000/50000 [==============================] - 19s 390us/step - loss: 0.1825 - acc: 0.9677 - val_loss: 0.5080 - val_acc: 0.8917\n",
            "Epoch 146/150\n",
            "50000/50000 [==============================] - 19s 389us/step - loss: 0.1818 - acc: 0.9690 - val_loss: 0.5172 - val_acc: 0.8955\n",
            "Epoch 147/150\n",
            "50000/50000 [==============================] - 20s 390us/step - loss: 0.1790 - acc: 0.9688 - val_loss: 0.5232 - val_acc: 0.8897\n",
            "Epoch 148/150\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 0.1813 - acc: 0.9681 - val_loss: 0.5272 - val_acc: 0.8878\n",
            "Epoch 149/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1835 - acc: 0.9681 - val_loss: 0.5281 - val_acc: 0.8896\n",
            "Epoch 150/150\n",
            "50000/50000 [==============================] - 19s 388us/step - loss: 0.1844 - acc: 0.9677 - val_loss: 0.5092 - val_acc: 0.8925\n",
            "Test loss:  0.5092341033458709\n",
            "Test accuracy:  0.8925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3xc5Znw/d+lUS9WtVwk25Irxja4\nYWpCTwwOBpI8LBCSkEZ2k2xInoQNpJDyPvtu9n022ZQFUggbQicsxQECpncMtrHBNsbdlizLktXr\naMr1/nEf2SNZtsdG45F0ru/nMx/PnHrNkee+zn2f+9xHVBVjjDH+lZLsAIwxxiSXJQJjjPE5SwTG\nGONzlgiMMcbnLBEYY4zPWSIwxhifs0RgfEVE/iwi/yfOZXeIyAWJjsmYZLNEYIwxPmeJwJhhSERS\nkx2DGTksEZghx2uSuUFE3hWRDhH5k4iMEZG/i0ibiDwrIoUxyy8VkfUi0iwiL4rIzJh580Rktbfe\nA0Bmv319QkTWeOu+LiInxRnjEhF5R0RaRaRKRH7Sb/5Z3vaavfnXetOzROQXIrJTRFpE5FVv2jki\nUj3AcbjAe/8TEXlIRO4WkVbgWhFZJCJvePvYIyL/JSLpMevPEpFnRKRRRPaKyPdFZKyIdIpIccxy\n80WkXkTS4vnuZuSxRGCGqk8BFwLTgUuAvwPfB0bj/t9+E0BEpgP3Ad/y5j0J/E1E0r1C8VHgLqAI\n+Ku3Xbx15wF3AF8FioHfA8tEJCOO+DqAzwEFwBLgn0TkMm+7k7x4f+vFNBdY4633H8AC4Awvpn8B\nonEek0uBh7x93gNEgG8DJcDpwPnA17wY8oBngaeA8cBU4DlVrQVeBK6I2e5ngftVNRRnHGaEsURg\nhqrfqupeVd0NvAKsUNV3VLUbeASY5y33D8ATqvqMV5D9B5CFK2hPA9KAX6lqSFUfAt6O2cd1wO9V\ndYWqRlT1TiDorXdYqvqiqr6nqlFVfReXjM72Zl8NPKuq93n7bVDVNSKSAnwRuF5Vd3v7fF1Vg3Ee\nkzdU9VFvn12qukpV31TVsKruwCWy3hg+AdSq6i9UtVtV21R1hTfvTuAaABEJAFfhkqXxKUsEZqja\nG/O+a4DPud778cDO3hmqGgWqgDJv3m7tO7Lizpj3k4DveE0rzSLSDEzw1jssETlVRF7wmlRagH/E\nnZnjbWPrAKuV4JqmBpoXj6p+MUwXkcdFpNZrLvp/44gB4DHgRBGpxNW6WlT1rWOMyYwAlgjMcFeD\nK9ABEBHBFYK7gT1AmTet18SY91XAv6pqQcwrW1Xvi2O/9wLLgAmqmg/8DujdTxUwZYB19gHdh5jX\nAWTHfI8ArlkpVv+hgm8DNgLTVHUUruksNobJAwXu1aoexNUKPovVBnzPEoEZ7h4ElojI+d7Fzu/g\nmndeB94AwsA3RSRNRD4JLIpZ94/AP3pn9yIiOd5F4Lw49psHNKpqt4gswjUH9boHuEBErhCRVBEp\nFpG5Xm3lDuCXIjJeRAIicrp3TWITkOntPw34IXCkaxV5QCvQLiInAP8UM+9xYJyIfEtEMkQkT0RO\njZn/F+BaYCmWCHzPEoEZ1lT1A9yZ7W9xZ9yXAJeoao+q9gCfxBV4jbjrCQ/HrLsS+ArwX0ATsMVb\nNh5fA34mIm3AzbiE1LvdXcDFuKTUiLtQfLI3+7vAe7hrFY3AvwMpqtribfN2XG2mA+jTi2gA38Ul\noDZcUnsgJoY2XLPPJUAtsBk4N2b+a7iL1KtVNba5zPiQ2INpjPEnEXkeuFdVb092LCa5LBEY40Mi\ncgrwDO4aR1uy4zHJZU1DxviMiNyJu8fgW5YEDFiNwBhjfM9qBMYY43PDbuCqkpISraioSHYYxhgz\nrKxatWqfqva/NwUYhomgoqKClStXJjsMY4wZVkTkkN2ErWnIGGN8zhKBMcb4nCUCY4zxuWF3jWAg\noVCI6upquru7kx1KQmVmZlJeXk5amj0/xBgzeEZEIqiuriYvL4+Kigr6DjQ5cqgqDQ0NVFdXU1lZ\nmexwjDEjyIhoGuru7qa4uHjEJgEAEaG4uHjE13qMMcffiEgEwIhOAr388B2NMcffiGgaMsaYw1KF\nwTyR6ukEFNJz+u4j2OrmpWVCJAxN26G9DlIzIKsQxsyCtKy+63Q2QvVbUPuem5aaAfnlUDAJAmmA\nQEaeWz9jFKQM/vm7JYJB0NzczL333svXvva1o1rv4osv5t5776WgoCBBkRkTh54OSM0EBJp3QuNW\nVzj1dEDlR6F4CuxZC6vvglAnBNKh4iw4YYkryGrfg84GCHdDWjbklLgCLBqGaMT921sQp2ZARj5k\njnKFW2qmm960A9Y+AO17YdblkF8Gq/8CNe/AqDLIzIfWGuiod9sMpMGY2TDuJFdoBtJhy3Ow7UXo\nqHOxl86EMXPcNqrfhmkfg498B7a/BG/eCt0tkJLmthVIh/Rsr6ANuJjTsiG7GCQFgm2Aunhb90Dd\nBje98iMuvt2roWELRI7w+OmUNCiaDOEut81gm9tXvC76v3Dqdcf+tz6EYTfo3MKFC7X/ncXvv/8+\nM2fOTFJEsGPHDj7xiU+wbt26PtPD4TCpqYOba5P9XYe9SNgVFgBTznU/elXoaXeFh6RAdgloBGrf\nhaad7qwvPRcyct0PuaPeneW173XrlC2Aiae6gq13Hy1V7mywtcZtL78MNAqhbleoZebD5qfh/b/B\nuLlwxj+7aU3b4YOn4IMnIRKCokoIdcG+za6QyS5x+wmkuZhyS10h1tngvRoPvA97+8opgeKpkDPa\nxdPZ6L4L4gr4thoXtwTc9+6vsMIV1GnZkFUEPW2uEE3Pdfs4moKsPwm4wjXU4eJJy3LJBtzfYuwc\naK+H7mYYNR5yx0BKqjvue9e7AnX/tlKg/BQomOi2uWetW2bMiTB+Hqx/1J2xA0y9wG07EvJePW6b\n3S3u75SS6uLobHD/PzJy3fZDXZBdBGUL3fwP/g5dje7/QOmJ7u+Rlu2Oi6S4Qj93jNt+ex3sXun+\nluk5LulkjnJn+uPnQ9l89/8r1On+/zRXecdWXcLoanKJeeycYzvUIqtUdeGA8ywRfHhXXnkljz32\nGDNmzCAtLY3MzEwKCwvZuHEjmzZt4rLLLqOqqoru7m6uv/56rrvOZfTe4TLa29u56KKLOOuss3j9\n9dcpKyvjscceIysr66B9Jfu7HrNI2P0IM/Nd4Xsk0agrlAJeV9m6jbD2XkjPc2eoxVPdD77ufdjx\nilsmZ7T7UXfUu1fnPldYFU1xP+6m7bDlWTcP3PqjZ7ofZ2fDgX0H0gE58tldfxneWW5b7cAF6kDy\nJ0LLLndcAukHYiud5QqIxm3uLHr0DFdIduxzSSsShmCLK1yiYXfmuv9V5P5NzXQFW1utO8vv2Ad5\n41xi6N3GmFnuzDkadoVXYQUUT3OFl4hLSFufh8qzYcG1kFXg/jY7XoH1j7gYy0+BUeMgkOEKsY59\n7vunpLq/tQTctjQK4aAr1LpbvGaUDjctpwRmf9rFvfEJl5xmf9ol0EOJhF2CaquB7laYeDrkFPdd\nJho90JTS2QjvPeQK7fIFR/e3HQF8lQh++rf1bKhpHdR9njh+FD++ZNYh58fWCF588UWWLFnCunXr\n9nfzbGxspKioiK6uLk455RReeukliouL+ySCqVOnsnLlSubOncsVV1zB0qVLueaaaw7a13FLBME2\nVyh1t7oCdO96V8hNvcAVwqEu1ySw6SlXJdbogVdKmvthSwo073I/1pZqVzhICuSOhYozYdzJ3rzd\nrgDKHQ07XoWdbxw408uf4M6ydq9yBcuAZ59C3+e6y4FCMdjmnfGKO6Msmw8nX+USxso/QdteKF/o\nCtr0HNfs0FrjYi1bACXT3XcNtnmFZ8glnNwxkOednVa95ZJJe70r3EaNd9+nsNIVZJ2N7vunBCA1\ny53dduxzBWjZfHccX/u1S3rlp8Dkc1yyi0fv79c6EpgjOFwisGsECbBo0aI+ff1/85vf8MgjjwBQ\nVVXF5s2bKS7ue+ZSWVnJ3LlzAViwYAE7duw4/E6ikcOfWYe63QUrcGdd219xBWKwzVVXyxZC3lhX\ngDRsdQX63vXufeM2184aS1JcIf/MzX2nB9KhZIaLJSXglgsH3bYiPe6su/wUmOOd7XU1ue1vexHe\n+6s7ix5VBttfds0DJdNh/ufcsqhLMk074NwfwClfdme5jdu86dvd2X7lR72z5Xo3P6uw77Hp6XRx\n9R6PXrMuO/wxjteUc93rUIomu2RzKONOgk//6dj2bQnADIIRlwgOd+Z+vOTkHOhJ8OKLL/Lss8/y\nxhtvkJ2dzTnnnDPgvQAZGRn73wcCAbo6Ow9cSMrIc9XrYKtrwvjVSdC621WFKz/qVurpcGeunQ3u\n7LlpO+SNd4XQ7lV921L37yjdFcSd+9zn3LHuTHT6x9x6eePc/PxyGH2Cawvd8hy017p20IKJ7uy1\nt238aKi6WLOLXWEWjbqmjqzCI687drZ79ZdfPvDy6dlHH58xPjLiEkEy5OXl0dY28BP/WlpaKCws\nJDs7m40bN/Lmm28eekMd+yDY7l2IbHVnvQCI1ywScmf6406GGRe73g8v/KtbJJDuCueMUe4M86Qr\noHE77PsA5n0GZi6Fkmmu+aN+k0sObd6FwzGzXA+QgomH/6Jp42H+Z4/+AA1ExDUf9UpJiS8JGGMG\nnSWCQVBcXMyZZ57J7NmzycrKYsyYMW5GqIvFZ5/G7275LTNnTGPGlEmctnCuK+xDna6ppaUamr0e\nHi1Vrn1dUlyhXjTZ1QS6m11zS3YZNGfAP9x1YOc9Xne+wFH8KSec4l7GGMMIvFicFBp1/0qKu5jY\n0+GaW4KxtQSva1ykp+8Fz5RU166dng2ZhW6Zw7T7Jv27GmOGJbtYnCjhoNdNscElg94LquAK+N42\n9pQUd6afEnDzg23uYm96jjubtwt+xpgkskQQL1XvppN217TT0+4+I65vdWqmO9MPpLuz+rScgW8F\nlxTXZ9wYY4YISwRHEmz3mnna3cVacO326bnuYmdmIaSmJzdGY4z5ECwR9BcNe7fnB113zFCnK/gz\nRkGGN9RA7/goxhgzAlgi6NU7cmBzlTvzT0l1zTz55W58lXiGRTDGmGHIEkF3K7Ttcf3zibqz/aLK\nvsPLGmPMCDZiHkxz1FTd6JGNW10PnpwSbxCyGUedBJqbm7n11luPKYxf/epXdHZ2HtO6xhgzGPyZ\nCKIRN7xwaw1kFrjCP7/swNjjR8kSgTFmOPNf01C4xw1aFu5y/fx7h9v9EG688Ua2bt3K3LlzufDC\nCyktLeXBBx8kGAxy+eWX89Of/pSOjg6uuOIKqquriUQi/OhHP2Lv3r3U1NRw7rnnUlJSwgsvvDBI\nX9IYY+I38hLB32888Mi3g6jrCaTetYCUOL/+2Dlw0c8POfvnP/8569atY82aNSxfvpyHHnqIt956\nC1Vl6dKlvPzyy9TX1zN+/HieeOIJwI1BlJ+fzy9/+UteeOEFSkpKDrl9Y4xJJB81Dakbz0cjR5cE\njtLy5ctZvnw58+bNY/78+WzcuJHNmzczZ84cnnnmGb73ve/xyiuvkJ9vN5UZY4aGkVcjONSZe1ut\n6x00qtw9ACVBVJWbbrqJr371qwfNW716NU8++SQ//OEPOf/887n55psH2IIxxhxf/qkRZBd5zzwd\n/CQQOwz1xz/+ce644w7a29sB2L17N3V1ddTU1JCdnc0111zDDTfcwOrVqw9a1xhjkiGhNQIRWQz8\nGggAt6vqz/vNnwTcAYwGGoFrVLU6IcEE0t2F4QSIHYb6oosu4uqrr+b0008HIDc3l7vvvpstW7Zw\nww03kJKSQlpaGrfddhsA1113HYsXL2b8+PF2sdgYkxQJG4ZaRALAJuBCoBp4G7hKVTfELPNX4HFV\nvVNEzgO+oKqHffLJkByG+jjy03c1xgyeww1DncimoUXAFlXdpqo9wP3Apf2WORF43nv/wgDzjTHG\nJFgiE0EZUBXzudqbFmst8Env/eVAnogU91sGEblORFaKyMr6+vqEBGuMMX6V7IvF3wXOFpF3gLOB\n3UCk/0Kq+gdVXaiqC0ePHvhi73B70tqx8MN3NMYcf4m8WLwbmBDzudybtp+q1uDVCEQkF/iUqjYf\n7Y4yMzNpaGiguLgYGaHDQ6sqDQ0NZGZmJjsUY8wIk8hE8DYwTUQqcQngSuDq2AVEpARoVNUocBOu\nB9FRKy8vp7q6mpHebJSZmUl5eXmywzDGjDAJSwSqGhaRbwBP47qP3qGq60XkZ8BKVV0GnAP8m4go\n8DLw9WPZV1paGpWVlYMUuTEmmVSV7lCUrPTDPwOkrrWb5zbWccnJ48nNGHn3xh5PCes+migDdR81\nxgw/0agiQp/m3Lq2br553zusrWrhi2dV8NWzpzAqM63Pem3dIR5aVc0vlm+iPRhmzKgMvvuxGUwe\nnUtLVw/3v1XF8xvrOGNqCZ8/fRLnzCglkDIym4yPxuG6j1oiMMYctc6eMI+v3cOEomxOm1yEiNDY\n0UNNcxcNHT2kpgj5WWnkZKSSFhBW7WzisTU1bKlrpysUoTMYpqMnQlFOOuefUMrssnxau0Lc9eZO\nWrtDnDGlhOc31pEiUJSTQVFOGplpAbpDETbXtaMKH5lWwjWnTeK/nt/Ce7tb9sdWlJPOBTNLeWlT\nPXtbg/zbJ+dw1aKJSTxaQ8PhEoHVp4wxcYlElY21rby8aR9/enUb+9p7AJhYlI2iVDV2HXb9soIs\nFlYUkp2eSnZ6gJyMVHY2dPDU+lr+usoNKDCtNJc7v7iImeNGsW53C8s37KW+LUhTRw/BcIRAinDx\nnHGcWlm8PwFdMHMMq3c10dkTISDCwopCMtMChCJRnl5fy9nTEze22EhhNQJjzGG9sbWBh1dX8/T6\nWlq7wwCcMaWYb54/jZrmLh5dU0NOeoC5EwqoKMmhOCedSFRp6gzRFQoTDEWpKMlhUUURKQM00fSE\no7R0hRiVlUpGqj0bPFGsRmCMOSbPvb+XL925kryMVC6cNYaPThvNgkmFTCjK3r/MJ+d/uJ5s6akp\njM7L+LChmg/BEoExZkBt3SF++Og6ZozJ49Gvn3nEXjxm+LJEYMwI0dUT4aFVVaytbqE4J52ywizm\nTyykMCed59/fy8baNiYWZTNtTC4LJhaRkZbCsrU1vLm1gSUnjeO8E0rZ197DO7uaKM5N56FVu6lt\n7ebWz8y3JDDCWSIw5jgKR6IEUmTAO+BbukL8/b09tHSFyM107eWBFPigtp3Xt+6jNC+Tf1k8g4Ks\nNO54bQdv72iktqWbcDTK2Pwsqho7aezooSQ3g9buED3haJ/t52Wk0hZ0bfwikJOeSnswTFZagIff\n2c3ovAzq24J91vnCmRXMm1iYuANihgRLBMYcB+FIlD++sp1fPbuJ0XkZnHdCKYsqizhx3Cg+qG3j\n6fW1PLW+lu5Q9KB1U1OEeRMLWLG9gcW/epnUlBTC0SgLK4o4tbKIQIqwp6WbRRVFfPGsSk6pcAX3\n7uYuVu1sor4tyDkzSplamktLV4j397Ty9vZGqpo6uXRuGYsqi/jb2hqeXl/LSeUFnFpZRFt3mPr2\nIEtPHn+8D5VJAus1ZEyCbd/XwfX3v8O71S1cMLMUEF7bso+u0IHxFQuy01gyZxxXnjKRKaU5tHeH\nCYajhCJRSkdlkpuRSmNHD797aSs94ShfOLOCScU5yftSZtixXkPGJMnf1tZw08PvkRoQbrl6PktO\nGge4LpMf1LaxrqaFiuIcTqkoJDVwYDDg7PSDf5pFOel8/2J7KJEZfJYIjEmA6qZOfva3DSzfsJf5\nEwv47dXzKSvI2j8/PTWFOeX5zCnPT2KUxjiWCIwZZDsbOrjo16+gCjd8fAbXfXQyaYFkP/rDmEOz\nRGDMIPufVdV0hyI8951zqCyxdnwz9NlpijGDSFV5bG0NZ0wpsSRghg1LBMYMonerW9jZ0MnSudbt\n0gwflgiMGUSPrakhPZDCx2eNTXYoxsTNrhEY3+r2+vFnph378AmhiOsGurW+ndG5GTz+bg3nnjCa\n/Ky0I69szBBhicCMeOt2t7CnpZvC7DTausNs2tvGiu2NvLZlH+mBFL5wZgXTxuRx74pd1LR0cfZ0\nN8ImQH1bkHd2NVPX1s2csgIqSrLZVt/Blrp2apq7qG7uOmgoh0vnliXjaxpzzOzOYjNk9I5zk5Ue\nIBJRWrtDvFPVzPqaFhZOKuKcGaPpCUfZtLeNyaNzyc9KozsU4fmNdexs6KS5swcF0gJCTkYquRmp\nPLWulte3Nhy0r0nF2Zw7o5S9rd38fV0tAOWFWUwfk8frW/f1GeqhrCCLMaMyWF/TSjAcJTs9wNTS\nXCYUZlNemMXssnymjcmlsb2HtmCYC2eOGXDcfWOSye4sNsdNJKqEItGDmltUlT0t3exs6GRjbSuv\nbN7H7qYuvn3hdM47oZT/88QG/vLGzgG3KQK/123kZaTS0RMmqq6wP6WiiPU1rbR0hQB3k1ZAhJ5I\nlEjUneCU5mXwg4tnsqiyiKbOHrLTU5k+JpeC7PT92/+gto2G9iCnTi4mkCJ09USoauokkCKMykzb\nP1Z+TzhKQ0eQMXmZVtCbEcVqBGZAkajSE466wvUIhV4oEmX7vg6Wr6/l3hW72NsWZOGkQuZPKiQt\nRahvD/Lypn3sbj7wKMOK4mzSAilsrmtnfH4mNS3dfO70SUwbk0dXT5jUlBQy0wLM8c62X9uyj+Xr\n9zImP5OZY/N4p6qZFzbWMX1sHlcvmsj8iYX7h0pWVbpD7qlXRTnppKdanwhj7OH15rBUlWc27KW8\nMJsTx4/iveoW/vHuVfsL7rKCLE6fUsz4/EzagxEKs9NYUFFIQ3sPD66sYsW2RnoirinlrKklzBo/\nipc372NjbSuqbvjjM6YWc8aUEiaPzmHy6FzKCrIIRaL89vktPPh2FT9YMpNLbKRLYxLGEoE5pFAk\nyo+XrefeFbsAOHv6aN7c1kBJbgZXnzqRUCTKxj1tvLm9gebOEDnpATpDEXr/25QVZHHxnLHMGp/P\nvIkFNiKmMUOUXSPwoQ9q21hT1cT2fZ2cOrmIj04bTXswzOqdTbxb3cL7e1rpCkXY29rNxto2vnr2\nZDICKfzp1e3MnVDArZ+ZT3HugefIqiqqkJIitHSGWL2riYzUFE6bXGzt5cYMc1YjGOaC4QjPbqhj\nxfYGTp9czPSxefz73zeyfMNewF1oVYX8rDRau0OoumkVxTmMykojNUW4etFEPrXAPYC8OxQhIzVl\nwCdoGWOGL6sRjBArtjXw6JoaCrPTyEgNsGFPC29tb6SpM0RaQPb3uslMS+G7H5vOxXPGMb4gixc/\nqOOZDXVMKMpiUWURJ5UXkJsx8J/+w9xcZYwZniwRDAMdwTC/WL6JO17bTk56gGA4SjiqVJbkcM6M\nUi6bV8bpk4t5a3sja6qauGxeGeWF2fvXXzx7HItnj0viNzDGDGWWCIawjbWt3LdiF/+zejftwTCf\nO30SN150ApmpLhn0dpfsdda0Es6aVpKkaI0xw1VCE4GILAZ+DQSA21X15/3mTwTuBAq8ZW5U1ScT\nGdNQpaq8tKmeFdsbae8O8251M2urW0gPpHDxnLF8/owK5k0s3L98/yRgjDHHKmGJQEQCwC3AhUA1\n8LaILFPVDTGL/RB4UFVvE5ETgSeBikTFNNSEIlE21LSyelcTf11ZzYY9raSmCLmZqZQXZnHzJ07k\nsnllFOWkH3ljxhhzjBJZI1gEbFHVbQAicj9wKRCbCBQY5b3PB2oSGM+QsXlvG3e/uZO/vbuHxo4e\nAKaW5vL/ffokLptbZnfCGmOOq0QmgjKgKuZzNXBqv2V+AiwXkX8GcoALBtqQiFwHXAcwceLEQQ/0\neIlGldtf3cZ/PL0JBC48cQyLZ41lwaRCxsc82NwYY46nZF8svgr4s6r+QkROB+4Skdmq2mdcX1X9\nA/AHcPcRJCHOY9YeDPOvT7zPqp2NNHb0sK+9h8WzxvKvl8/uc8OWMcYkSyITwW5gQszncm9arC8B\niwFU9Q0RyQRKgLoExnXcbKxt5Wt3r2ZHQwfnnVDK3AkFnDm1hKUnj7cbtowxQ0YiE8HbwDQRqcQl\ngCuBq/stsws4H/iziMwEMoH6BMZ03CxbW8O/PLSWvMw07vnyaZw+pTjZIRljzIASlghUNSwi3wCe\nxnUNvUNV14vIz4CVqroM+A7wRxH5Nu7C8bU63Ma8iKGqvL+njXtW7OSeFbtYOKmQW6+ZT2leZrJD\nM8aYQ0roNQLvnoAn+027Oeb9BuDMRMZwvGypa+fbD6zhvd0tBFKEz58+iR8sOdF6ABljhrxkXywe\nER5bs5ubHn6PzLQA/89ls1kyZ5z1/TfGDBuWCD6Exo4efvTYOp54dw+nVBTy26vmMzbfmoGMMcOL\nJYJjtGpnE1+9axUtXT3c8PEZfPWjk0kNWDOQMWb4sURwDB55p5rvPfQe4woyuetLZzFz3Kgjr2SM\nMUOUJYKj9Pi7NXz7gbWcNrmI2z6zgEK7FmCMGeYsERyFtVXNfOfBtSyYVMidX1xERqqNAGqMGf6s\nUTtO+9qDfOUvKxmdl8HvP7vAkoAxZsSwGkEcVJXvP/wezZ0hHvvGmZTYGEHGmBHEagRxeHj1bpZv\n2Mt3Pz7dLgwbY0YcSwRHUNfWzU+WrWdRRRFfOmtyssMxxphBZ4ngCG5/ZTsdPWF+/qk5BFJsxFBj\nzMgTVyIQkYdFZImI+CpxNHb0cPebO1l68ngmj85NdjjGGJMQ8Rbst+KGkN4sIj8XkRkJjGnI+NOr\n2+gKRfjGeVOTHYoxxiRMXIlAVZ9V1c8A84EdwLMi8rqIfEFE0hIZYLK0dIX4y+s7uXj2OKaW5iU7\nHGOMSZi4m3pEpBi4Fvgy8A7wa1xieCYhkSXZPSt20hYM87VzpyQ7FGOMSai47iMQkUeAGcBdwCWq\nuseb9YCIrExUcMkSDEf482s7+Mi0EmaNz092OMYYk1Dx3lD2G1V9YaAZqrpwEOMZEh5bU0NdW5Bf\nXHFyskMxxpiEi7dp6EQRKej9ICKFIvK1BMWUVKrKH1/exglj8zhrakmywzHGmISLNxF8RVWbez+o\nahPwlcSElFxvbGtgc107X8kSuQUAABFBSURBVPnIZETsvgFjzMgXbyIISEypKCIBYESOv/z4u3vI\nTg+w5KRxyQ7FGGOOi3ivETyFuzD8e+/zV71pI0o4EuXpdbWcd0IpmWk2uqgxxh/iTQTfwxX+/+R9\nfga4PSERJdFb2xtp6OhhyRyrDRhj/COuRKCqUeA27zViPfHeHrLSApwzozTZoRhjzHET730E04B/\nA04EMnunq+qIGY4zElWeXl/LeTNLyUq3ZiFjjH/Ee7H4v3G1gTBwLvAX4O5EBZUMK3c0sq/dmoWM\nMf4TbyLIUtXnAFHVnar6E2BJ4sI6/t7a3gjAmVPs3gFjjL/Ee7E46A1BvVlEvgHsBkbUuMyrdzUx\ntTSX/OwROYaeMcYcUrw1guuBbOCbwALgGuDzR1pJRBaLyAciskVEbhxg/n+KyBrvtUlEmgfaTqKp\nKu9UNTN/YsGRFzbGmBHmiDUC7+axf1DV7wLtwBfi2bC33i3AhUA18LaILFPVDb3LqOq3Y5b/Z2De\n0YU/OLbv66C5M8T8iYXJ2L0xxiTVEWsEqhoBzjqGbS8CtqjqNlXtAe4HLj3M8lcB9x3Dfj601btc\nRWT+JEsExhj/ifcawTsisgz4K9DRO1FVHz7MOmVAVcznauDUgRYUkUlAJfD8IeZfB1wHMHHixDhD\njt/qXU3kZaYy1R5HaYzxoXgTQSbQAJwXM02BwyWCo3El8JBX+ziIqv4B+APAwoULdZD2ud/qnU3M\nnVBAij2c3hjjQ/HeWRzXdYF+dgMTYj6Xe9MGciXw9WPYx4fWHgyzaW8bH5s1Nhm7N8aYpIv3zuL/\nxtUA+lDVLx5mtbeBaSJSiUsAVwJXD7DtE4BC4I14Yhls71Y1E1Wsx5AxxrfibRp6POZ9JnA5UHO4\nFVQ17N1z8DQQAO5Q1fUi8jNgpaou8xa9ErhfVQe9ySce79e2AdgjKY0xvhVv09D/xH4WkfuAV+NY\n70ngyX7Tbu73+SfxxJAoW+vbyc9KoyR3RD5ewRhjjijeG8r6mwaMiCE6t9a1M7U0155GZozxrXiv\nEbTR9xpBLe4ZBcPe1vp2zjthROQ0Y4w5JvE2DeUlOpBkaO7sYV97D1Ps/gFjjI/F1TQkIpeLSH7M\n5wIRuSxxYR0fW+vdvXFTSy0RGGP8K95rBD9W1ZbeD6raDPw4MSEdP1vr2gGsRmCM8bV4E8FAy8Xb\n9XTI2lrfTnoghfLCrGSHYowxSRNvIlgpIr8UkSne65fAqkQGdjxsqWunsiSH1MCxdp4yxpjhL94S\n8J+BHuAB3Cii3SRpSIjBtLW+nSmlOckOwxhjkireXkMdwEEPlhnOguEIuxo7ueTk8ckOxRhjkire\nXkPPiEhBzOdCEXk6cWEl3o59nUTVegwZY0y8TUMlXk8hAFS1iWF+Z/HWetdjaHKJJQJjjL/Fmwii\nIrL/iTAiUsEAo5EOJ3taugGYUGQ9howx/hZvF9AfAK+KyEuAAB/Be2LYcFXfFiQtIORnpSU7FGOM\nSap4LxY/JSILcYX/O8CjQFciA0u0+rYgJbkZNticMcb34h107svA9binjK0BTsM9SOa8w603lNW3\nBxmdl5HsMIwxJunivUZwPXAKsFNVzwXmAc2HX2Voq28LMjrXEoExxsSbCLpVtRtARDJUdSMwI3Fh\nJV59m9UIjDEG4r9YXO3dR/Ao8IyINAE7ExdWYkWiSmOHJQJjjIH4LxZf7r39iYi8AOQDTyUsqgRr\n6AgSVSwRGGMMxzCCqKq+lIhAjqf6tiCAXSMwxhiO/ZnFw9r+RGA1AmOMsURgjDF+58tEsK+9B4AS\naxoyxhh/JoL6tiA56QFyMob9Q9aMMeZD82cisLuKjTFmP38mgrZuSwTGGOPxaSKwGoExxvTybyKw\nC8XGGAMkOBGIyGIR+UBEtojIgM88FpErRGSDiKwXkXsTGQ9AdyhCa3fYagTGGONJWLcZEQkAtwAX\nAtXA2yKyTFU3xCwzDbgJOFNVm0Qk4Y+/3Ndu9xAYY0ysRNYIFgFbVHWbqvYA9wOX9lvmK8At3jOQ\nUdW6BMYD2M1kxhjTXyITQRlQFfO52psWazowXUReE5E3RWTxQBsSketEZKWIrKyvr/9QQR0YZyjz\nQ23HGGNGimRfLE4FpgHnAFcBf/SGu+5DVf+gqgtVdeHo0aM/1A7331Wcl/6htmOMMSNFIhPBbmBC\nzOdyb1qsamCZqoZUdTuwCZcYEqazJwxgdxUbY4wnkYngbWCaiFSKSDpwJbCs3zKP4moDiEgJrqlo\nWwJjojsUASAjNdmVIWOMGRoSVhqqahj4BvA08D7woKquF5GfichSb7GngQYR2QC8ANygqg2Jigkg\nGI4iAukBSwTGGAMJ7D4KoKpPAk/2m3ZzzHsF/rf3Oi66QxEyUlMQkeO1S2OMGdJ8d1ocDEfJTAsk\nOwxjjBkyfJcIemsExhhjHN+ViFYjMMaYvnyXCKxGYIwxffmuRLQagTHG9OW7RGA1AmOM6ct3JaLV\nCIwxpi/fJYLuUJSMVEsExhjTy3eJIBiOkJHmu69tjDGH5LsSMRiKkmk1AmOM2c9/icBqBMYY04fv\nSsRuqxEYY0wfPkwEViMwxphYvioRw5Eo4ahajcAYY2L4KhEEw1EAqxEYY0wMX5WIvYkg0+4sNsaY\n/XxVIu5/TKXdWWyMMfv5KhHsrxFY05AxxuznqxKxt0ZgF4uNMeYAXyUCu1hsjDEH81WJaDUCY4w5\nmK8SgdUIjDHmYL4qEff3GrIagTHG7OerRGC9howx5mC+KhGtRmCMMQfzVSKwawTGGHMwX5WIwd5e\nQ3ZnsTHG7JfQRCAii0XkAxHZIiI3DjD/WhGpF5E13uvLiYznQNOQr/KfMcYcVmqiNiwiAeAW4EKg\nGnhbRJap6oZ+iz6gqt9IVByxguEoIpAesERgjDG9ElkiLgK2qOo2Ve0B7gcuTeD+jqg7FCEjNQUR\nSWYYxhgzpCQyEZQBVTGfq71p/X1KRN4VkYdEZMJAGxKR60RkpYisrK+vP+aAguGoXR8wxph+kt1G\n8jegQlVPAp4B7hxoIVX9g6ouVNWFo0ePPuaddYciNryEMcb0k8hEsBuIPcMv96btp6oNqhr0Pt4O\nLEhgPATDUes6aowx/SSyVHwbmCYilSKSDlwJLItdQETGxXxcCryfwHisRmCMMQNIWK8hVQ2LyDeA\np4EAcIeqrheRnwErVXUZ8E0RWQqEgUbg2kTFA1YjMMaYgSQsEQCo6pPAk/2m3Rzz/ibgpkTGEMtq\nBMYYczBfnR5bjcAYYw7mq1KxOxS1AeeMMaYfXyWCYDhiNQJjjOnHV6ViMBS1awTGGNOPvxKB1QiM\nMeYgvioVu61GYIwxB/FZIrAagTHG9OebUjEciRKOqtUIjDGmH98kAntwvTHGDMw3peL+5xXb08mM\nMaYP35SK3fa8YmOMGZBvEsH+GoE1DRljTB++KRX31wjsYrExxvThm0RgNQJjjBmYb0pFqxEYY8zA\nfJMIrEZgjDED802p2FsjsGGojTGmL98kAruhzBhjBuabUtFqBMYYMzDfJIIDNQJLBMYYE8s/iaC3\nRmBNQ8YY04dvSsWJRdlcNHusdR81xph+UpMdwPHysVlj+dissckOwxhjhhzf1AiMMcYMzBKBMcb4\nnCUCY4zxOUsExhjjc5YIjDHG5ywRGGOMz1kiMMYYn7NEYIwxPieqmuwYjoqI1AM7j3H1EmDfIIaT\nCBbj4LAYB8dQj3GoxwdDJ8ZJqjp6oBnDLhF8GCKyUlUXJjuOw7EYB4fFODiGeoxDPT4YHjFa05Ax\nxvicJQJjjPE5vyWCPyQ7gDhYjIPDYhwcQz3GoR4fDIMYfXWNwBhjzMH8ViMwxhjTjyUCY4zxOd8k\nAhFZLCIfiMgWEbkx2fEAiMgEEXlBRDaIyHoRud6bXiQiz4jIZu/fwiTHGRCRd0Tkce9zpYis8I7l\nAyKSnuT4CkTkIRHZKCLvi8jpQ/AYftv7G68TkftEJDPZx1FE7hCROhFZFzNtwOMmzm+8WN8VkflJ\njPH/en/rd0XkEREpiJl3kxfjByLy8WTFGDPvOyKiIlLifU7KcTwSXyQCEQkAtwAXAScCV4nIicmN\nCoAw8B1VPRE4Dfi6F9eNwHOqOg14zvucTNcD78d8/nfgP1V1KtAEfCkpUR3wa+ApVT0BOBkX65A5\nhiJSBnwTWKiqs4EAcCXJP45/Bhb3m3ao43YRMM17XQfclsQYnwFmq+pJwCbgJgDvt3MlMMtb51bv\nt5+MGBGRCcDHgF0xk5N1HA/LF4kAWARsUdVtqtoD3A9cmuSYUNU9qrrae9+GK8DKcLHd6S12J3BZ\nciIEESkHlgC3e58FOA94yFsk2fHlAx8F/gSgqj2q2swQOoaeVCBLRFKBbGAPST6Oqvoy0Nhv8qGO\n26XAX9R5EygQkXHJiFFVl6tq2Pv4JlAeE+P9qhpU1e3AFtxv/7jH6PlP4F+A2B45STmOR+KXRFAG\nVMV8rvamDRkiUgHMA1YAY1R1jzerFhiTpLAAfoX7zxz1PhcDzTE/xGQfy0qgHvhvr/nqdhHJYQgd\nQ1XdDfwH7sxwD9ACrGJoHcdehzpuQ/U39EXg7977IROjiFwK7FbVtf1mDZkYY/klEQxpIpIL/A/w\nLVVtjZ2nrn9vUvr4isgngDpVXZWM/ccpFZgP3Kaq84AO+jUDJfMYAnjt7JfiktZ4IIcBmhKGmmQf\ntyMRkR/gmlfvSXYssUQkG/g+cHOyY4mXXxLBbmBCzOdyb1rSiUgaLgnco6oPe5P39lYXvX/rkhTe\nmcBSEdmBa047D9ceX+A1cUDyj2U1UK2qK7zPD+ESw1A5hgAXANtVtV5VQ8DDuGM7lI5jr0MdtyH1\nGxKRa4FPAJ/RAzdDDZUYp+CS/lrvt1MOrBaRsQydGPvwSyJ4G5jm9dJIx11QWpbkmHrb2/8EvK+q\nv4yZtQz4vPf+88Bjxzs2AFW9SVXLVbUCd8yeV9XPAC8An052fACqWgtUicgMb9L5wAaGyDH07AJO\nE5Fs72/eG+OQOY4xDnXclgGf83q9nAa0xDQhHVcishjXXLlUVTtjZi0DrhSRDBGpxF2Qfet4x6eq\n76lqqapWeL+damC+9391yBzHPlTVFy/gYlwPg63AD5IdjxfTWbiq97vAGu91Ma4d/jlgM/AsUDQE\nYj0HeNx7Pxn3A9sC/BXISHJsc4GV3nF8FCgcascQ+CmwEVgH3AVkJPs4AvfhrlmEcIXVlw513ADB\n9bzbCryH6wGVrBi34NrZe38zv4tZ/gdejB8AFyUrxn7zdwAlyTyOR3rZEBPGGONzfmkaMsYYcwiW\nCIwxxucsERhjjM9ZIjDGGJ+zRGCMMT5nicCY40hEzhFvFFdjhgpLBMYY43OWCIwZgIhcIyJvicga\nEfm9uGcytIvIf3rPFXhOREZ7y84VkTdjxsfvHcN/qog8KyJrRWS1iEzxNp8rB56fcI93t7ExSWOJ\nwJh+RGQm8A/Amao6F4gAn8ENFrdSVWcBLwE/9lb5C/A9dePjvxcz/R7gFlU9GTgDd/cpuFFmv4V7\nNsZk3LhDxiRN6pEXMcZ3zgcWAG97J+tZuMHXosAD3jJ3Aw97z0MoUNWXvOl3An8VkTygTFUfAVDV\nbgBve2+parX3eQ1QAbya+K9lzMAsERhzMAHuVNWb+kwU+VG/5Y51fJZgzPsI9js0SWZNQ8Yc7Dng\n0yJSCvuf4zsJ93vpHS30auBVVW0BmkTkI970zwIvqXviXLWIXOZtI8Mbp96YIcfORIzpR1U3iMgP\ngeUikoIbVfLruIfeLPLm1eGuI4Abrvl3XkG/DfiCN/2zwO9F5GfeNv7XcfwaxsTNRh81Jk4i0q6q\nucmOw5jBZk1Dxhjjc1YjMMYYn7MagTHG+JwlAmOM8TlLBMYY43OWCIwxxucsERhjjM/9/xBHVci/\nWY7DAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXpi_2GBdDPV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "235ITQu-dCXo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_idKnl--N5r0"
      },
      "source": [
        "# **Explications de la démarche** :\n",
        "\n",
        "##I. Chargement des données de cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "A partir de cette ligne de code, on obtient :\n",
        "- l'ensemble des données d'entraînement (x_train et y_train)\n",
        "- l'ensemble des données de test (x_test et y_test)\n",
        "\n",
        "Ces deux ensembles seront nécessaires à la suite de TP.\n",
        "\n",
        "##II. Convertion en float32\n",
        "\n",
        "Il est commun d'utiliser une précision de 32 bit pour entraîner un réseau de neuronnes, et comme nous l'avions fait également en TD, nous avons donc converti l'ensemble d'entraînement en 32 bit floats.\n",
        "\n",
        "##III. Normalisation de l'entrée de [0; 255] à [0; 1]\n",
        "\n",
        "Par rapport à la division par 255, c'est la valeur maximale qu'un bit peut prendre (le type de l’entrée avant la conversion en float32), cela va donc transformer les données d'entrée pour les mettre dans des valeurs entre 0.0 et 1.0. \n",
        "\n",
        "On préfère généralement avoir des fonctionnalités d’entrée sur cette échelle afin que le taux d’apprentissage par défaut (et d’autres hyperparamètres) fonctionnent raisonnablement bien.\n",
        "\n",
        "##V. Préparation pour la couche neuronnale\n",
        "\n",
        "Nous avons initialisé une variable num_classes à 10, puisqu'il y a 10 classes par rapport à cifar10, et cette variable nous sera utile quand il faudra créer la dernière couche neuronnal de sortie avec la fonction d'activation softmax.\n",
        "\n",
        "Les keras.utils.to_categorical des y_train et y_test sont nécessaires pour convertir ces vecteurs de classe (type entier) en matrices de classe binaire. On en a besoin impérativement pour l'utilisation de categorical_crossentropy (fonction de perte) dans notre couche de neuronnes.\n",
        "\n",
        "##VI. Learning rate\n",
        "\n",
        "Nous avons implémenté une fonction lr_schedule qui prend en paramètre le nombre d'epochs actuels et qui sera utilisé en \"callback\" pour le model.fit_generator (entraînement du réseau avec data augmentation plus tard). En fonction du nombre d'epochs, on changera le learning rate, en le minimisant vers la fin pour avoir la meilleure précision possible.\n",
        "\n",
        "##VII. Le réseau : CNN\n",
        "\n",
        "###**Couches principales**\n",
        "\n",
        "Nous avons créé au total six couches de convolution :\n",
        "- Les deux premières avec 32 neuronnes.\n",
        "- Les deux suivantes avec 64 neuronnes.\n",
        "- Les deux dernières avec 128 neuronnes.\n",
        "\n",
        "Chacune des couches de convolution a une fonction d'activation ELU (Exponential Linear Unit), une autre dérivé de la ReLU. Celle-ci va approcher les valeurs moyenne proche de 0, ce qui va avoir comme impact d’améliorer les performances d’entrainements. Après plusieurs tests entre ELU et ReLU, et au vu des résultats, nous avons gardé elu malgré la rapidité de ReLU.\n",
        "\n",
        "Ces couches ont toutes en deuxième paramètre le tuple (3,3), ce qui veut dire que l'on teste à chaque fois des zones 3x3 par rapport aux images.\n",
        "\n",
        "###**Utilisation de weight_decay (kernel regularizer L2) :**\n",
        "\n",
        "La régularisation du poids (weight_decay) fournit une approche pour réduire \"l'overfitting\" d’un modèle de réseau neuronal d’apprentissage sur les données d'entraînement et permet d’améliorer le rendement du modèle sur les nouvelles données, comme l’ensemble de test.\n",
        "\n",
        "Il existe plusieurs types de régularisation du poids, telles que les normes vectorielles L1 et L2, et chacune nécessite un hyperparamètre qui doit être configuré. Nous avons choisi le vecteur L2 après plusieurs tests entre les différents vecteurs, et avons gardé l'hyper-paramètre 1e-4 par rapport aux différents tests et aux performances observées.\n",
        "\n",
        "Chacune des couches de convolution utilise le kernel regulizer (L2). La norme vectorielle L2 est la Ridge Regression. \n",
        "\n",
        "###**BatchNormalisation**\n",
        "\n",
        "Pour chacune des couches on utilise un BatchNormalisation, comme vu en TD. \n",
        "\n",
        "BatchNormalization normalise l’activation de la couche précédente à chaque lot, c.-à-d. qu'il applique une transformation qui maintient l’activation moyenne près de 0 et l’écart-type d’activation près de 1. Elle règle le problème du décalage des covariables internes. Il agit également comme un regularizer, dans certains cas, éliminant la nécessité du Dropout. La normalisation par lots atteint la même précision avec moins d’étapes d'entraînement, ce qui accélère le processus d'entraînement\n",
        "\n",
        "###**MaxPooling2D**\n",
        "\n",
        "Toutes les deux couches de convolution, on utilise la couche MaxPooling2D\n",
        "\n",
        "L’opération de mutualisation maximale peut être ajoutée à l’exemple travaillé en ajoutant la couche MaxPooling2D fournie par l’API Keras. \n",
        "\n",
        "L’objectif est d’échantillonner vers le bas une représentation d’entrée (image, matrice de sortie de couche cachée, etc.), en réduisant sa dimensionnalité et en permettant d’émettre des hypothèses sur les caractéristiques contenues dans les \"sub-regions binned\".\n",
        "\n",
        "Ceci est fait en partie pour aider à l'overfitting en fournissant une forme abstraite de la représentation. En outre, il réduit le coût de calcul en réduisant le nombre de paramètres à apprendre et fournit une invariance de traduction de base à la représentation interne.\n",
        "\n",
        "###**Dropout**\n",
        "\n",
        "Toutes les deux couches de convolution, et après MaxPooling2D, on réalise un Dropout dont les hyperparamètres ont été changés plusieurs fois jusqu'aux hyperparamètres actuels pour avoir les meilleurs résultats possibles.\n",
        "\n",
        "Nous avons appris que \"l'overfitting\" est un problème, et pour pallier à ce problème, il existe la fonction \"Dropout\". \n",
        "\n",
        "L’idée clé est de \"laisser tomber\" aléatoirement les neuronnes (avec leurs connexions) du réseau neuronal pendant l’entraînement. La réduction du nombre de paramètres à chaque étape de la formation a un effet de régularisation, et donc aide à réduire \"l'overfitting\".\n",
        "\n",
        "###**Couche de sortie**\n",
        "\n",
        "Avant la couche de sortie, nous avons réalisé un Flatten(). Cela permet d'ajouter une couche « aplatie » qui prépare un vecteur, nécessaire pour la couche de sortie composé de 10 neuronnes (num_classes de cifar10) avec la fonction d'activation softmax (cf l'image ci-dessous pour l'exemple).\n",
        "\n",
        "![Après un flatten](https://missinglink.ai/wp-content/uploads/2019/04/Group-5.png)\n",
        "\n",
        "###**Compile**\n",
        "\n",
        "Nous avons créé notre propre optimizer avec la fonction rmsprop de keras.optimizers, avec un learning rate de 0.001 et un decay de 1e-6. Nous avions utilisé adam auparavant, mais après plusieurs tests d'optimizers, et en changeant de multiples fois les paramètres de rmsprop, nous avons gardé cette configuration d'optimizer car c'est avec celle-ci que nous avons obtenu les meilleurs résultats.\n",
        "\n",
        "Ensuite, nous avons fait un model.compile en ayant une fonction de perte qui est categorical_crossentropy, et avec l'optimizer stipulé précédemment.\n",
        "\n",
        "##VIII. Early stopper\n",
        "\n",
        "Nous avons ajouté un Early stopper, qui permet d'arrêter le processus d'entraînement si la \"loss\" commence à augmenter à un certain moment.\n",
        "\n",
        "##IX. Data augmentation\n",
        "\n",
        "Dans Keras, nous avons une classe ImageDataGenerator qui est utilisée pour générer des lots de données d’image de tenseur avec augmentation des données en temps réel. Les données seront mises en boucle (par lots) indéfiniment. Les données d’image sont générées en transformant les images d’entraînement par rotation, coupes, décalages zoom, retournements, réflexion, normalisation, etc.\n",
        "\n",
        "Nous avons gardé ces hyperparamètres après avoir testé plusieurs data augmentation avec des hyperparamètres différents.\n",
        "\n",
        "###X. Entraînements\n",
        "\n",
        "Nous avons deux phases d'entraînement, l'une avec data augmentation, l'autre avec un entraînement normal. Le batch_size, ainsi que le nombre d'epochs, ont été modifiés à chaque fois jusqu'à arriver à des résultats concluants.\n",
        "\n",
        "### Phase 1 : entraînement avec data augmentation\n",
        "\n",
        "Nous avons réalisé tout d'abord un premier entraînement avec data augmentation, en utilisant évidemment le datagen créé précédemment, en prenant un pas par epoch de x_train.shape[0], soit la taille de x_train, un batch_size de 32, et le learning rate de lr_schedule qui changera en fonction du nombre d'epochs. Ceci nous permet d'aller jusqu'à une accuracy (acc) d'environ 90%, et une val_acc d'environ 0.87 en moyenne.\n",
        "\n",
        "### Phase 2 : entraînement normal\n",
        "\n",
        "Nous avons remarqué qu'avec un deuxième entraînement, celui-ci \"normal\" car sans data augmentation, et après celui de la data augmentation, on pouvait augmenter sensiblement l'accuracy et la val_acc. Pour preuve, nous avons réussi à aller jusqu'à 96% pour l'accuracy, et val_acc jusqu'à presque 0.90. Le nombre d'epochs a été augmenté jusqu'à 150 (et peut-être augmenté sûrement pour augmenter l'accuracy) pour avoir ces résultats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdXR8FXwb9a0"
      },
      "source": [
        ""
      ]
    }
  ]
}